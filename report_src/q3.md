1. It seems that smaller batch size (e.g. 1) results in slightly higher running time than bigger batch sizes (e.g. 100). Slower learning rates similarly result in slightly higher running time as well, although all are not significant changes. 

2. It seems that larger batches and slower learning rates result in better final parameters, although not entirely consistent. 

3. The final smooth loss seems to decrease with larger batchsizes. It also seems to work best with mixed learning rates, but in general, it seems that the final loss is lower with slower learning rate 