{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(batchsize):\n",
    "    def getbatch(key):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        context = 10*jax.random.uniform(subkey, [batchsize])\n",
    "        key, subkey = jax.random.split(key)\n",
    "        output = 3 + 10 * context - 0.7 * context**2 + jax.random.normal(subkey, [batchsize])\n",
    "        return context, output\n",
    "    return getbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen = get_data_generator(100)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "context, output = gen(key)\n",
    "plt.plot(context, output, 'o')\n",
    "plt.xlabel('context')\n",
    "plt.ylabel('output')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jax\n",
    "from jax import flatten_util\n",
    "from jax import numpy as jnp\n",
    "from typing import Callable\n",
    "def signed_gradient_descent(net: Callable, loss: Callable, getbatch: Callable, max_iter: int, learning_rates: list[int], *params):\n",
    "    assert isinstance(net,Callable)\n",
    "    assert isinstance(loss,Callable)\n",
    "    assert isinstance(getbatch,Callable)\n",
    "    assert isinstance(max_iter,int)\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    w, unflatten = jax.flatten_util.ravel_pytree(params)\n",
    "\n",
    "    # batch predictive network over context, but not params\n",
    "    batched_net = jax.vmap(net,[0]+[None]*len(params))\n",
    "    # batch loss over both context and predictions\n",
    "    batched_loss = jax.vmap(loss)\n",
    "\n",
    "    def l(key,w):\n",
    "        params = unflatten(w)\n",
    "        context, next = getbatch(key)\n",
    "        pred = batched_net(context,*params)\n",
    "        return jnp.mean(batched_loss(pred, next))\n",
    "\n",
    "    fun = jax.value_and_grad(l,1)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    avg_grad = 0\n",
    "    avg_loss = 0\n",
    "    smooth_loss = 0\n",
    "\n",
    "    print(\" iter     l.r.     loss (smooth)    (avg)     time\")\n",
    "    print(\"----- -------- -------- -------- -------- --------\")\n",
    "   \n",
    "    for i in range(max_iter):\n",
    "        n = int(i * len(learning_rates) / max_iter)\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        loss, grad = fun(subkey, w)\n",
    "        avg_loss = (i*avg_loss + loss)/(i+1)\n",
    "        alpha = max(.01,1/(i+1))\n",
    "        smooth_loss = alpha*loss + (1-alpha) * smooth_loss\n",
    "\n",
    "        if i % (max_iter // 20) == 0:\n",
    "            print(f\"{i: >5} {learning_rates[n]:8.5f} {loss:8.5f} {smooth_loss:8.5f} {avg_loss:8.5f} {time.time()-t0:8.5f}\")\n",
    "        avg_grad = 0.9 * avg_grad + 0.1 * grad\n",
    "\n",
    "        w = w - learning_rates[n]*jnp.sign(avg_grad)\n",
    "   \n",
    "    t = time.time()-t0\n",
    "    t = round(t * 1000, 5)\n",
    "\n",
    "    if len(params)==1:\n",
    "        params = unflatten(w)[0]\n",
    "        # print(f\"  {t}   | {params[0]}  | {params[1]} | {params[2]} | {avg_loss} \")\n",
    "        return params\n",
    "    else:\n",
    "        params = unflatten(w)\n",
    "        # print(f\"  {t}   | {params[0]}  | {params[1]} | {params[2]} | {avg_loss} \")\n",
    "        return unflatten(w)\n",
    "\n",
    "def net(context,a,b,c):\n",
    "    return a + b*context + c*context**2\n",
    "\n",
    "def loss(pred, output):\n",
    "    return (pred - output)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time (ms)   |       a       |      b     |       c     |   final smooth loss\n",
      "--------------|---------------|------------|-------------|---------------------\n",
      "  2605.96681   | [3.6941566]  | [7.0941534] | [-0.9058423] | 303.3845520019531 \n",
      "  2494.78602   | [3.4941568]  | [9.494158] | [-0.70584226] | 66.2212905883789 \n",
      "  2495.23616   | [2.6941576]  | [10.294161] | [-0.5058422] | 50.71369552612305 \n",
      "  2482.02205   | [5.504189]  | [8.924267] | [-0.59584194] | 112.87616729736328 \n",
      "  2494.96484   | [3.0341573]  | [10.014162] | [-0.6858425] | 31.885662078857422 \n"
     ]
    }
   ],
   "source": [
    "# arrays of batchsize, max_iter, learning_rates\n",
    "batchsizes = [1,10,100,100,100]\n",
    "max_iters = 1001\n",
    "learning_rates = [[.1],[.1],[.1],[.01], [.1,.01]]\n",
    "\n",
    "print(\"  Time (ms)   |       a       |      b     |       c     |   final smooth loss\")\n",
    "print(\"--------------|---------------|------------|-------------|---------------------\")\n",
    "\n",
    "for batchsize, learning_rate in zip(batchsizes, learning_rates):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    a = jax.random.normal(key, 1)\n",
    "    b = jax.random.normal(key, 1)\n",
    "    c = jax.random.normal(key, 1)\n",
    "    params = [a,b,c]\n",
    "\n",
    "    gen = get_data_generator(batchsize)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "   \n",
    "    params = signed_gradient_descent(net, loss, gen, max_iters, learning_rate, *params)\n",
    "    print(f\"  {params} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It seems that smaller batch size (e.g. 1) results in slightly higher running time than bigger batch sizes (e.g. 100). Slower learning rates similarly result in slightly higher running time as well, although all are not significant changes. \\\n",
    "2. It seems that larger batches and slower learning rates result in better final parameters, although not entirely consistent. \\\n",
    "3. The final smooth loss seems to decrease with larger batchsizes. It also seems to work best with mixed learning rates, but in general, it seems that the final loss is lower with slower learning rate \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Epic of Saul, by William Cleaver Wilkinson\n"
     ]
    }
   ],
   "source": [
    "chars = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '!', '?', ':', '\"', \"'\", '+', ',', '.', ' ', '\\n']\n",
    "start = [55, 17, 14, 70, 51, 27, 24, 19, 14, 12, 29, 70, 42, 30, 29, 14, 23,\n",
    "       11, 14, 27, 16, 70, 40, 37, 24, 24, 20, 70, 24, 15, 70, 55, 17, 14,\n",
    "       70, 40, 25, 18, 12, 70, 24, 15, 70, 54, 10, 30, 21, 68, 70, 11, 34,\n",
    "       70, 58, 18, 21, 21, 18, 10, 22, 70, 38, 21, 14, 10, 31, 14, 27, 70,\n",
    "       58, 18, 21, 20, 18, 23, 28, 24, 23]\n",
    "\n",
    "res = ''.join([chars[c] for c in start])\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.load('data.npz',mmap_mode='r')['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(context_size, batch_size):\n",
    "    def getbatch(key):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        start = jax.random.randint(subkey, shape=batch_size, minval=0, maxval=len(data)-context_size)\n",
    "        indices = start[:,None] + jnp.arange(context_size)[None,:]\n",
    "        context = data[indices] # (batchsize x context_size)\n",
    "        next = data[start+context_size] # (batchsize,)\n",
    "        return context, next\n",
    "    return getbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of context:  (5, 50) Shape of next:  (5,)\n",
      "context:  [[24 23 28 25 18 27 10 12 34 68 71 14 33 29 14 23 13 14 13 70 11 34 70 29\n",
      "  17 14 70 36 12 29 28 70 24 15 70  1  8  8  7 68 70  1  8  9  1 68 70 10\n",
      "  23 13]\n",
      " [18 29 14 13 70 54 29 10 29 14 28 70 12 24 25 34 27 18 16 17 29 70 18 23\n",
      "  70 29 17 14 28 14 70 32 24 27 20 28 68 70 28 24 70 29 17 14 70 41 24 30\n",
      "  23 13]\n",
      " [28 69 70 44 29 70 18 28 70 25 27 24 11 10 11 21 14 70 29 17 10 29 70 39\n",
      "  30 27 17 10 22 71 32 10 28 70 27 14 10 21 21 34 70 22 24 27 14 70 18 23\n",
      "  15 21]\n",
      " [23 70 29 17 14 70 56 23 18 29 14 13 70 54 29 10 29 14 28 68 70 32 14 70\n",
      "  13 24 70 23 24 29 70 12 21 10 18 22 70 10 70 27 18 16 17 29 70 29 24 70\n",
      "  25 27]\n",
      " [10 28 70 32 14 21 21 69 65 71 71 65 44 70 32 10 28 70 16 24 18 23 16 70\n",
      "  29 24 70 10 28 20 70 34 24 30 70 29 24 70 21 14 29 70 22 14 70 29 10 20\n",
      "  14 70]]\n",
      "next:  [70 10 30 14 10]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "getbatch = get_data_generator(50, 5)\n",
    "context, next = getbatch(key)\n",
    "\n",
    "print(\"Shape of context: \", context.shape, \"Shape of next: \", next.shape)\n",
    "\n",
    "print(\"context: \", context)\n",
    "\n",
    "print(\"next: \", next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\\\n",
    "context shape: (5, 50). (batchsize, context_size) \\\n",
    "next shape: (5,) (batchsize) \\\n",
    "\n",
    "b)\n",
    "context contains the batched numerical data that act as the input (\"previous knowledge\") to predict the target values contained in next. \\\n",
    "\n",
    "c)\n",
    "the key is split into 2 subkeys, one for the context and one for next, such that we dont reuse the same random variables when generating.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from jax.nn import logsumexp\n",
    "\n",
    "def loss(pred,next):\n",
    "    num_characters = len(chars)\n",
    "    assert pred.shape == (num_characters,)\n",
    "    assert next.shape == ()\n",
    "    log_probs = pred - logsumexp(pred)\n",
    "    assert log_probs.shape == (num_characters,)\n",
    "    next_onehot = jax.nn.one_hot(next, num_classes=num_characters)\n",
    "    assert next_onehot.shape == (num_characters,)\n",
    "    out = -jnp.sum(log_probs * next_onehot)\n",
    "    assert out.shape == ()\n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def constant_net(context, b):\n",
    "    # input context is a 1-D array of size context_size\n",
    "    # each entry is an index between 0 and num_characters\n",
    "    # these represent the most recent characters\n",
    "    (context_size,) = context.shape\n",
    "    (num_characters,) = b.shape\n",
    "\n",
    "    # [do stuff.]\n",
    "\n",
    "    # predict a constant vector\n",
    "    pred = b\n",
    "\n",
    "          \n",
    "    assert pred.shape == (num_characters,)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter     l.r.     loss (smooth)    (avg)     time\n",
      "----- -------- -------- -------- -------- --------\n",
      "    0  0.00100  4.27667  4.27667  4.27667  0.02772\n",
      "  500  0.00100  3.75796  3.84823  4.00168  2.72620\n",
      " 1000  0.00100  3.42981  3.48416  3.79038  5.43914\n",
      " 1500  0.00100  3.28311  3.28676  3.63863  8.39022\n",
      " 2000  0.00100  3.18296  3.18663  3.53152 11.02354\n",
      " 2500  0.00100  3.13428  3.14404  3.45595 13.64159\n",
      " 3000  0.00100  3.13118  3.13523  3.40261 16.30306\n",
      " 3500  0.00100  3.14047  3.13310  3.36422 19.26403\n",
      " 4000  0.00100  3.10767  3.13320  3.33532 21.99465\n",
      " 4500  0.00100  3.11254  3.13212  3.31281 24.82710\n",
      " 5000  0.00010  3.12061  3.13259  3.29482 27.55235\n",
      " 5500  0.00010  3.11545  3.13263  3.28018 30.22116\n",
      " 6000  0.00010  3.14931  3.13334  3.26787 33.03475\n",
      " 6500  0.00010  3.15036  3.13409  3.25748 35.96461\n",
      " 7000  0.00010  3.14825  3.13339  3.24855 38.84472\n",
      " 7500  0.00010  3.11208  3.13300  3.24085 41.47183\n",
      " 8000  0.00010  3.14472  3.13200  3.23414 44.18035\n",
      " 8500  0.00010  3.11545  3.13456  3.22823 47.28741\n",
      " 9000  0.00010  3.14127  3.13248  3.22299 50.03408\n",
      " 9500  0.00010  3.13086  3.13249  3.21829 52.93438\n"
     ]
    }
   ],
   "source": [
    "batchsize = 4096\n",
    "context_size = 32\n",
    "iters = 10000\n",
    "learning_rates = [.001, .0001]\n",
    "b = jnp.zeros(len(chars))\n",
    "\n",
    "params = [b]\n",
    "\n",
    "gen = get_data_generator(context_size, batchsize)\n",
    "\n",
    "params = signed_gradient_descent(constant_net, loss, gen, iters, learning_rates, *params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "char2int = dict(zip(chars, range(len(chars))))\n",
    "\n",
    "def generate_char(net,context,key,*params):\n",
    "    pred = net(context,*params);\n",
    "    assert pred.shape == (num_characters,)\n",
    "    out = jax.random.categorical(key, pred);\n",
    "    assert out.shape == ()\n",
    "    return out\n",
    "\n",
    "def generate(net,context_str,context_size,num_char,*params):\n",
    "    context = [char2int[c] for c in context_str]\n",
    "\n",
    "    key = jax.random.PRNGKey(1)\n",
    "    for i in range(num_char):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        my_context = jnp.array(context[-context_size:])\n",
    "        c = generate_char(net,my_context,subkey,*params)\n",
    "        context.append(int(c))\n",
    "\n",
    "    out = ''.join([chars[i] for i in context])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDENT:\n",
      "I have searched the skies and found...\n",
      "\n",
      "TEACHER:\n",
      "Yes? What have you found?\n",
      "\n",
      "STUDENT\n",
      "I have found a fact, a fact most excellent.\n",
      "  c rtttu\n",
      " e e\n",
      "lv ib,grunek  el iertah d\n",
      "  pauo goghmf e  lrC\"ac  ty\n",
      "eb tte  biic,nitn iohp\n",
      "oooiotttracvenn   ern..ch I uen th ee\n",
      "ha alelr   woteInupo t  r w\n",
      "ro\"e.lr\n",
      "b ubDepegyttloi.hsinnceOfimtwd\n",
      "wc,ernreluebn tnl dpcer sis  ds reo \"doh a ne c.satsfnm lsa rnf rioefhaorcodees heotG e ao\n",
      "ohtle dmhe  eC ecsd2c,aride  eei Th?ordr\n",
      "Paehaeel l ud 0hrsoiad e eiavewi mr\n",
      "dtf   s fyahiSs eIn p oic olo\n",
      " d r a Re mmth r leSess dto seiia  ielmrt hedluii2e s \n",
      "liy ,frocn\n",
      "  fmce g dBndctud .iod ,agty  l .L shEp\n"
     ]
    }
   ],
   "source": [
    "b_constant = params\n",
    "start = \"STUDENT:\\nI have searched the skies and found...\\n\\nTEACHER:\\nYes? What have you found?\\n\\nSTUDENT\\nI have found a fact, a fact most excellent.\\n\"\n",
    "num_characters = len(chars)\n",
    "print(generate(constant_net,start,context_size,500,b_constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def linear_net(context, b, W):\n",
    "    # W, b = params[0], params[1]\n",
    "    # print('w: ', W)\n",
    "    # print('b: ', b)\n",
    "\n",
    "    (context_size,) = context.shape\n",
    "\n",
    "    (context_size2, num_characters, num_characters2) = W.shape\n",
    "    assert context_size == context_size2\n",
    "    assert num_characters == num_characters2\n",
    "\n",
    "    context_onehot = jax.nn.one_hot(context, num_classes=num_characters)\n",
    "    assert context_onehot.shape == (context_size, num_characters)\n",
    "\n",
    "\n",
    "    # We need to multiply context_onehot with W along the context_size dimension\n",
    "    \n",
    "    pred = jnp.einsum('ij,ijk->k', context_onehot, W) + b\n",
    "\n",
    "    assert pred.shape == (num_characters,)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter     l.r.     loss (smooth)    (avg)     time\n",
      "----- -------- -------- -------- -------- --------\n",
      "    0  0.00100  4.28394  4.28394  4.28394  0.16214\n",
      "  500  0.00100  2.56146  2.63390  2.84003  7.86799\n",
      " 1000  0.00100  2.37643  2.39678  2.64487 15.21716\n",
      " 1500  0.00100  2.33781  2.31454  2.54092 22.31885\n",
      " 2000  0.00100  2.29808  2.28205  2.47820 29.77769\n",
      " 2500  0.00100  2.26396  2.27183  2.43770 37.18627\n",
      " 3000  0.00100  2.28718  2.27311  2.41000 44.95227\n",
      " 3500  0.00100  2.24690  2.27569  2.39074 52.18431\n",
      " 4000  0.00100  2.29151  2.28119  2.37678 59.62810\n",
      " 4500  0.00100  2.28138  2.28403  2.36655 67.86923\n",
      " 5000  0.00010  2.28246  2.29063  2.35896 75.23881\n",
      " 5500  0.00010  2.30287  2.28773  2.35269 83.16335\n",
      " 6000  0.00010  2.29197  2.28485  2.34715 93.40257\n",
      " 6500  0.00010  2.35776  2.28828  2.34241 106.17691\n",
      " 7000  0.00010  2.31598  2.28488  2.33819 114.35264\n",
      " 7500  0.00010  2.24745  2.28746  2.33473 125.59524\n",
      " 8000  0.00010  2.26777  2.28614  2.33171 135.56565\n",
      " 8500  0.00010  2.27731  2.28815  2.32907 143.07084\n",
      " 9000  0.00010  2.32643  2.28784  2.32678 152.37469\n",
      " 9500  0.00010  2.31722  2.28497  2.32467 159.80256\n"
     ]
    }
   ],
   "source": [
    "batchsize = 4096\n",
    "context_size = 32\n",
    "iters = 10000\n",
    "learning_rates = [.001, .0001]\n",
    "num_characters = len(chars)\n",
    "\n",
    "b_linear = jnp.zeros(num_characters)\n",
    "W_linear = jnp.array(.01*np.random.randn(context_size, num_characters, num_characters))\n",
    "\n",
    "\n",
    "params = [b_linear, W_linear]\n",
    "\n",
    "gen = get_data_generator(context_size, batchsize)\n",
    "\n",
    "params = signed_gradient_descent(linear_net, loss, gen, iters, learning_rates, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDENT:\n",
      "I have searched the skies and found...\n",
      "\n",
      "TEACHER:\n",
      "Yes? What have you found?\n",
      "\n",
      "STUDENT\n",
      "I have found a fact, a fact most excellent.\n",
      "\n",
      " of the   s bllige the Amby whe probe d\n",
      " \"Prup of hmuthare thacllay\n",
      "d were  or canit the pestory: \"Arcutwa cherragr at tenginee,\n",
      "Ithale, as woted upolt  haverobe.\n",
      "\n",
      "\n",
      "N CLDecaly tiol. Sin centim, of caringelleby tho dpaknes somes aiof dof aree cosstichm ls orne tion anorsedens he ther ablotele! meer ond cow ton ine beem The rom\n",
      "Painatel ufuthengsoind coniag witho\n",
      "d mante foreple ton proflyono\n",
      "Mack are, mather theessadlougenin sizemethind uinne s inis ffrocher fice of\n",
      "Bricturn. He tasty he wall be\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = params\n",
    "start = \"STUDENT:\\nI have searched the skies and found...\\n\\nTEACHER:\\nYes? What have you found?\\n\\nSTUDENT\\nI have found a fact, a fact most excellent.\\n\"\n",
    "\n",
    "print(generate(linear_net,start,context_size,500, *p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def mlp_net(context, b, c, W, V):\n",
    "    assert context.shape == (context_size,)\n",
    "    (context_size,) = context.shape\n",
    "    (num_characters,) = b.shape\n",
    "    (num_hidden,) = c.shape\n",
    "    (num_characters, num_hidden) = W.shape\n",
    "    (context_size, num_characters, num_hidden) = V.shape\n",
    "\n",
    "    context_onehot = jax.nn.one_hot(context, num_classes=num_characters)\n",
    "    assert context_onehot.shape == (context_size, num_characters)\n",
    "\n",
    "    # [do stuff]\n",
    "\n",
    "    # first layer\n",
    "    h1 = jnp.einsum('ij,ijk->k', context_onehot, V) + c\n",
    "    l1 = jax.nn.relu(h1)\n",
    "    \n",
    "\n",
    "    pred = b + W @ l1\n",
    "    assert pred.shape == (num_characters,)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter     l.r.     loss (smooth)    (avg)     time\n",
      "----- -------- -------- -------- -------- --------\n",
      "    0  0.00100  4.27603  4.27603  4.27603  0.79735\n",
      "  500  0.00100  1.90563  2.00908  2.30095 34.99069\n",
      " 1000  0.00100  1.78632  1.80122  2.06997 71.84578\n",
      " 1500  0.00100  1.74954  1.73723  1.96433 112.49686\n",
      " 2000  0.00100  1.74241  1.70301  1.90110 156.91322\n",
      " 2500  0.00100  1.62852  1.67725  1.85782 194.31828\n",
      " 3000  0.00100  1.66574  1.65953  1.82540 233.58647\n",
      " 3500  0.00100  1.65879  1.64913  1.80061 270.97481\n",
      " 4000  0.00100  1.63123  1.64054  1.78076 310.07613\n",
      " 4500  0.00100  1.58940  1.62880  1.76425 349.32883\n",
      " 5000  0.00010  1.62587  1.62300  1.75032 386.42197\n",
      " 5500  0.00010  1.64957  1.59733  1.73701 425.49937\n",
      " 6000  0.00010  1.61687  1.58785  1.72477 463.36210\n",
      " 6500  0.00010  1.62156  1.58363  1.71394 499.46647\n",
      " 7000  0.00010  1.58306  1.57478  1.70408 538.08612\n",
      " 7500  0.00010  1.52659  1.57148  1.69539 577.97836\n",
      " 8000  0.00010  1.56541  1.56741  1.68757 614.37243\n",
      " 8500  0.00010  1.56117  1.56740  1.68047 649.60792\n",
      " 9000  0.00010  1.54836  1.56470  1.67399 685.35523\n",
      " 9500  0.00010  1.62733  1.55928  1.66808 720.54154\n"
     ]
    }
   ],
   "source": [
    "batchsize = 4096\n",
    "context_size = 32\n",
    "iters = 10000\n",
    "learning_rates = [.001, .0001]\n",
    "num_characters = len(chars)\n",
    "\n",
    "num_hidden = 500\n",
    "b_mlp = jnp.zeros(num_characters)\n",
    "c_mlp = jnp.zeros(num_hidden)\n",
    "W_mlp = jnp.array(.01*np.random.randn(num_characters, num_hidden))\n",
    "V_mlp = jnp.array(.01*np.random.randn(context_size, num_characters, num_hidden))\n",
    "\n",
    "\n",
    "params = [b_mlp, c_mlp, W_mlp, V_mlp]\n",
    "\n",
    "gen = get_data_generator(context_size, batchsize)\n",
    "\n",
    "params = signed_gradient_descent(mlp_net, loss, gen, iters, learning_rates, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDENT:\n",
      "I have searched the skies and found...\n",
      "\n",
      "TEACHER:\n",
      "Yes? What have you found?\n",
      "\n",
      "STUDENT\n",
      "I have found a fact, a fact most excellent.\n",
      "\n",
      "To retture    liver, restor elexpress delierso of the Daymarce! The\n",
      "difficulting, it will how to the cuttains one. Wat feeling mark, took the Vedaup on a swork at leagure which it grain confided two,\n",
      "the lutby that parts, successed by\n",
      "him he cousts of liverning obland convested the family were began anced boaring being to rell and and leud andsomal conicient of\n",
      "the Volvania, satingphanced of discard, mather leters who said, as immorded upon the\n",
      "like, by heart clught becauded. The stock fastshop\n"
     ]
    }
   ],
   "source": [
    "p = params\n",
    "start = \"STUDENT:\\nI have searched the skies and found...\\n\\nTEACHER:\\nYes? What have you found?\\n\\nSTUDENT\\nI have found a fact, a fact most excellent.\\n\"\n",
    "\n",
    "print(generate(mlp_net,start,context_size,500, *p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def dbl_net(context, b, c, d, W, V, U):\n",
    "    assert context.shape == (context_size,)\n",
    "\n",
    "    context_onehot = jax.nn.one_hot(context, num_classes=num_characters)\n",
    "    assert context_onehot.shape == (context_size, num_characters)\n",
    "\n",
    "    # [do stuff]\n",
    "    h1 = jnp.einsum('ijk,ij->k', V, context_onehot) + c\n",
    "    l1 = jax.nn.relu(h1)\n",
    "    \n",
    "    # second layer\n",
    "    h2 = jnp.einsum('k,jk->k', l1, U) + d\n",
    "    l2 = jax.nn.relu(h2)\n",
    "\n",
    "    pred = b + W @ l2\n",
    "    assert pred.shape == (num_characters,)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter     l.r.     loss (smooth)    (avg)     time\n",
      "----- -------- -------- -------- -------- --------\n",
      "    0  0.00100  4.27667  4.27667  4.27667  0.39515\n",
      "  500  0.00100  1.91986  1.98242  2.21543 52.13944\n",
      " 1000  0.00100  1.84668  1.86097  2.04942 102.83609\n",
      " 1500  0.00100  1.85455  1.82285  1.97715 152.54611\n",
      " 2000  0.00100  1.82844  1.80457  1.93525 202.87642\n",
      " 2500  0.00100  1.76737  1.79316  1.90756 253.00644\n",
      " 3000  0.00100  1.77197  1.78332  1.88719 306.35678\n",
      " 3500  0.00100  1.78004  1.78175  1.87214 356.96288\n",
      " 4000  0.00100  1.76319  1.77826  1.86052 407.42856\n",
      " 4500  0.00100  1.74391  1.77386  1.85109 456.78786\n",
      " 5000  0.00010  1.76011  1.77274  1.84338 504.86405\n",
      " 5500  0.00010  1.76384  1.72043  1.83286 554.39541\n",
      " 6000  0.00010  1.74332  1.71038  1.82284 601.72159\n",
      " 6500  0.00010  1.74147  1.70693  1.81395 649.82745\n",
      " 7000  0.00010  1.72367  1.69977  1.80580 700.99020\n",
      " 7500  0.00010  1.64946  1.69591  1.79862 751.55216\n",
      " 8000  0.00010  1.67967  1.69293  1.79215 804.38640\n",
      " 8500  0.00010  1.70911  1.69330  1.78629 854.83338\n",
      " 9000  0.00010  1.68200  1.69000  1.78097 903.49232\n",
      " 9500  0.00010  1.74276  1.68659  1.77616 957.79651\n"
     ]
    }
   ],
   "source": [
    "batchsize = 4096\n",
    "context_size = 32\n",
    "iters = 10000\n",
    "learning_rates = [.001, .0001]\n",
    "num_characters = len(chars)\n",
    "\n",
    "num_hidden = 500\n",
    "b_dbl = jnp.zeros(num_characters)\n",
    "c_dbl = jnp.zeros(num_hidden)\n",
    "d_dbl = jnp.zeros(num_hidden)\n",
    "W_dbl = jnp.zeros((num_characters, num_hidden))\n",
    "V_dbl = jnp.array(.01*np.random.randn(context_size, num_characters, num_hidden))\n",
    "U_dbl = jnp.array(.01*np.random.randn(num_hidden, num_hidden))\n",
    "\n",
    "params = [b_dbl, c_dbl, d_dbl, W_dbl, V_dbl, U_dbl]\n",
    "\n",
    "gen = get_data_generator(context_size, batchsize)\n",
    "\n",
    "params = signed_gradient_descent(dbl_net, loss, gen, iters, learning_rates, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDENT:\n",
      "I have searched the skies and found...\n",
      "\n",
      "TEACHER:\n",
      "Yes? What have you found?\n",
      "\n",
      "STUDENT\n",
      "I have found a fact, a fact most excellent.\n",
      "\n",
      "To rettle s breation oundral, for the deticalong gome that Came bar\n",
      "But Jew Timc, it with and to the cuncuess on. You duen the mark, though world upon a way\n",
      "rountly bevered by thod.\"\n",
      "\n",
      "\"He word,work at the Cebjects Rock, with discendiders\n",
      "where.\n",
      "\n",
      " Scrusist ruserious arroodeds here was overwanted a both cown't wite been breard'\n",
      "    One in where so, I canifus into\n",
      "dufter Royaging to pastic plopt they be maintailed scant, such found other to him stilike, by he had lightened up of detage man which\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = params\n",
    "start = \"STUDENT:\\nI have searched the skies and found...\\n\\nTEACHER:\\nYes? What have you found?\\n\\nSTUDENT\\nI have found a fact, a fact most excellent.\\n\"\n",
    "\n",
    "print(generate(dbl_net,start,context_size,500, *p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter     l.r.     loss (smooth)    (avg)     time\n",
      "----- -------- -------- -------- -------- --------\n",
      "    0  0.00100  4.27667  4.27667  4.27667  0.53196\n",
      "  500  0.00100  1.95496  2.00869  2.18941 231.11978\n",
      " 1000  0.00100  1.90694  1.91072  2.06173 479.21931\n",
      " 1500  0.00100  1.92144  1.90963  2.01003 728.60522\n",
      " 2000  0.00100  1.89641  1.87344  1.97817 972.28603\n",
      " 2500  0.00100  1.83292  1.86724  1.95542 1235.81153\n",
      " 3000  0.00100  1.85139  1.85567  1.93868 1502.43324\n",
      " 3500  0.00100  1.86184  1.85394  1.92677 1766.35529\n",
      " 4000  0.00100  1.85162  1.86234  1.91908 2033.64448\n",
      " 4500  0.00100  1.80985  1.87569  1.91336 2294.00349\n",
      " 5000  0.00010  1.84876  1.86237  1.90806 2556.69526\n",
      " 5500  0.00010  1.82357  1.78007  1.89734 2820.77900\n",
      " 6000  0.00010  1.79365  1.76830  1.88672 3096.17514\n",
      " 6500  0.00010  1.79828  1.76144  1.87713 3365.67538\n",
      " 7000  0.00010  1.76979  1.75612  1.86842 3650.80706\n",
      " 7500  0.00010  1.71450  1.75216  1.86071 3917.23336\n",
      " 8000  0.00010  1.72298  1.74601  1.85365 4184.19163\n",
      " 8500  0.00010  1.76215  1.74602  1.84727 4467.21299\n",
      " 9000  0.00010  1.74628  1.74563  1.84152 7570.98535\n",
      " 9500  0.00010  1.78409  1.73857  1.83623 7794.31486\n"
     ]
    }
   ],
   "source": [
    "batchsize = 4096\n",
    "context_size = 32\n",
    "iters = 10000\n",
    "learning_rates = [.001, .0001]\n",
    "num_characters = len(chars)\n",
    "\n",
    "num_hidden = 2500\n",
    "b_dbl = jnp.zeros(num_characters)\n",
    "c_dbl = jnp.zeros(num_hidden)\n",
    "d_dbl = jnp.zeros(num_hidden)\n",
    "W_dbl = jnp.zeros((num_characters, num_hidden))\n",
    "V_dbl = jnp.array(.01*np.random.randn(context_size, num_characters, num_hidden))\n",
    "U_dbl = jnp.array(.01*np.random.randn(num_hidden, num_hidden))\n",
    "\n",
    "\n",
    "params = [b_dbl, c_dbl, d_dbl, W_dbl, V_dbl, U_dbl]\n",
    "\n",
    "gen = get_data_generator(context_size, batchsize)\n",
    "\n",
    "params = signed_gradient_descent(dbl_net, loss, gen, iters, learning_rates, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDENT:\n",
      "I have searched the skies and found...\n",
      "\n",
      "TEACHER:\n",
      "Yes? What have you found?\n",
      "\n",
      "STUDENT\n",
      "I have found a fact, a fact most excellent.\n",
      "\n",
      "The they we bwlitied to the slexervicil\n",
      "    No goghmen. We Ccame by the Jew bish, it his passon to the know serval hak benother,\n",
      "have elrast other polit way robe. Befoue\n",
      "boways ind.\"335 Oblowers,\" carred lief \"And Rpcers, sumpassed the barness.\n",
      "\n",
      " Scrust!\" And the fairrs Mrss herood for beale on the Caprod William He may Mord'\n",
      "Pheianes leuded thou will if still sonfented angios to pastic of the Charless the readess who sevent in mull had in ploy\n",
      "like, bove o't clugidenchaud mind togeth was, shop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = params\n",
    "start = \"STUDENT:\\nI have searched the skies and found...\\n\\nTEACHER:\\nYes? What have you found?\\n\\nSTUDENT\\nI have found a fact, a fact most excellent.\\n\"\n",
    "\n",
    "print(generate(dbl_net,start,context_size,500, *p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
